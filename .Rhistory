<<<<<<< HEAD
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
# many columns are Nas
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
head(H4A)
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(20)) # only 20 befcause it takes a while
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
library(MASS)
library(caret)
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(20)) # only 20 befcause it takes a while
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
library(MASS)
library(caret)
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(20)) # only 20 befcause it takes a while
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
library(MASS)
library(caret)
Accuracies <- c(0.00)
# best QDFA model:
# H4A$PopSex ~ GOL + XCB + OBH + XML + FOL + VRR
for (i in seq(20)) # only 20 befcause it takes a while
{
inTrain <- createDataPartition(y = H4A$PopSex, # y = grouping variable, stratified random split
## the outcome data are needed
p = .70, ## The proportion of records in the training set
list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
# testrecs <- as.numeric(rownames(testing)) # row(testing)[,1]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
knn4
knn4_pred
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(20))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
knn4
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(20))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(20))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "knn",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.k = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
Accuracies <- c(0.00)
for (i in seq(20))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
knn4
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
=======
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
<<<<<<< HEAD
knn4 <- train(PopSex ~ ., data = training, method = "Linda",
>>>>>>> b94920f3bcc5e5b3d822fa04d8a719d5008d5651
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
<<<<<<< HEAD
knn4
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
#Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "logreg",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
# many columns are Nas
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(10))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(500))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
plot(density(Accuracies))
knn4
confusionMatrix(knn4_pred,testing$PopSex)
confusionMatrix(knn4)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
=======
plot(density(Accuracies))
knn4
knn4 <- train(PopSex ~ ., data = training, method = "Linda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
=======
pda4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(pda4, list(.lambda = 2))
pda4_pred <- predict(pda4,newdata = testing)
Accuracies[i] <- confusionMatrix(pda4_pred,testing$PopSex)$overall["Accuracy"]
}
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
>>>>>>> c366098756d68c37760d2af766458b056d04171a
>>>>>>> b94920f3bcc5e5b3d822fa04d8a719d5008d5651
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
<<<<<<< HEAD
library(MASS)
library(caret)
Accuracies <- c(0.00)
for (i in seq(5))
{
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "pda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
update(knn4, list(.lambda = 3))
knn4_pred <- predict(knn4,newdata = testing)
Accuracies[i] <- confusionMatrix(knn4_pred,testing$PopSex)$overall["Accuracy"]
}
summary(Accuracies)
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
require(rpart)
# caret function
inTest <- createDataPartition(H4A$PopSex, p = .20, list = FALSE)
# subset = holdout record numbers, NOT using train function from caret
chdr <- rpart(PopSex ~ .,
data=H4A,
method = "class", subset = inTest,
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.control(usesurrogate = 0, maxsurrogate = 0))
library("caret")
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
require(rpart)
# caret function
inTest <- createDataPartition(H4A$PopSex, p = .20, list = FALSE)
# subset = holdout record numbers, NOT using train function from caret
chdr <- rpart(PopSex ~ .,
data=H4A,
method = "class", subset = inTest,
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.control(usesurrogate = 0, maxsurrogate = 0))
confusionMatrix(H4A[inTest,"Popsex"],predict(chdr,newdata = H4A[inTest,], type = "class") )
confusionMatrix(SSU[inTest,"chd"],predict(chdr,newdata = SSU[inTest,], type = "class") )
confusionMatrix(H4A[inTest,"Popsex"],predict(chdr,newdata = H4A[inTest,], type = "class") )
Howells <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/Howells.csv', as.is = T);
attach(Howells);
# this time get ALL predictors
HBNMF <- Howells[which(Pop == 'NORSE' | Pop == 'BERG'),];
H4A <- na.omit(HBNMF[,c(5:61,63,67:80)])
H4A$PopSex <- as.factor(H4A$PopSex)
table(H4A$PopSex)
require(rpart)
# caret function
inTest <- createDataPartition(H4A$PopSex, p = .20, list = FALSE)
# subset = holdout record numbers, NOT using train function from caret
chdr <- rpart(PopSex ~ .,
data=H4A,
method = "class", subset = inTest,
parms = list(split = "gini", prior = c(1/4,1/4,1/4,1/4)),
control = rpart.control(usesurrogate = 0, maxsurrogate = 0))
confusionMatrix(H4A[inTest,"Popsex"],predict(chdr,newdata = H4A[inTest,], type = "class") )
chdr
library(MASS) # contains the data
library(caret) #select tuning parameters
#library(e1071) #SVM
set.seed(7); #replicable across our PCs? no.
# South African heart disease data
sahdd <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/sahdd.csv', as.is = T);
# sahdd$chd <- as.factor(sahdd$chd)
# exclude row number (first column)
# scale all continuous predictors, leave out famhist
SS <- cbind(sahdd["chd"], scale(sahdd[,c(-1,-6,-11)]) )
# SSU includes family history
SSU <- sahdd[,c(2:11)]
SS[,"chd"] <- as.factor(SS[,"chd"])
SSU[,"chd"] <- as.factor(SSU[,"chd"])
SSU[,"famhist"] <- as.factor(SSU[,"famhist"])
# or SS$chd <- as.factor(SS$chd)
chdr <- rpart(chd ~ .,
data=SSU,
method = "class", subset = inTest,
parms = list(split = "gini"),
control = rpart.control(usesurrogate = 0, maxsurrogate = 0))
confusionMatrix(SSU[inTest,"chd"],predict(chdr,newdata = SSU[inTest,], type = "class") )
chdir
chdr
Sens <- c(0.00)
for (i in seq(10)) # takes a while
{
inTrain <- createDataPartition(SSU$chd, p = .75, list = FALSE) # compact now
training <- SSU[inTrain,]
test <- SSU[-inTrain,]
HHP <- train(chd~., data = training,
method = "rpart",
control = rpart.control(minsplit = 20, cp = 0.01)) # bootstrap
#trControl = trainControl(method = "cv"))
cm <- confusionMatrix(SSU[-inTrain,"chd"], predict(HHP, newdata = SSU[-inTrain,]))
Sens[i] <- cm$table[1,1] / (cm$table[1,1] + cm$table[2,1])
}
summary(Sens) # not great
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
#attach(eddat)
#six group
dm6 <- morpho[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
#attach(eddat)
#six group
dm6 <- eddat[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
setwd("C:/Users/pnkum/Desktop/Project/Advance-Analytical-Technique")
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
#six group
dm6 <- eddat[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])
dm$group <- as.factor(dm$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
=======
<<<<<<< HEAD
library(MASS)
library(caret)
Accuracies <- c(0.00)
inTrain <- createDataPartition(y = H4A$PopSex, p = .70, list = FALSE)
training <- H4A[inTrain,]
testing <- H4A[-inTrain,]
knn4 <- train(PopSex ~ ., data = training, method = "Linda",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = trainControl(method = "cv"))
qhp <- read.csv("qhp.csv")
qhp$metal_level <- as.factor(qhp$metal_level)
cleandataqhp <- qhp[c(4,24, 26:62)]
dim(cleandataqhp)
cqhp <- cbind(cleandataqhp["metal_level"], scale(cleandataqhp[2:39]))
require(MASS)
require(caret)
require(nnet)
Accuracies <- c(0.00)
for (i in seq(1))
>>>>>>> b94920f3bcc5e5b3d822fa04d8a719d5008d5651
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:10)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 10)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'African American' | group == 'Asian American'| group == 'European America'| group == 'Native American'| group == 'CA Hispanic Amer'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:10)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 50)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
<<<<<<< HEAD
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))
=======
plot(density(na.omit(Accuracies)), main = paste('Neural Network accuracies:',acclen, 'runs') )
=======
setwd("C:/Users/Nucleus/Desktop/Project/Advance-Analytical-Technique")
altcrime<-read.csv(altcrime.csv)
altcrime<-read.csv("altcrime.csv")
head(altcrime)
plot(as.factor(altcrime$crime)), altcrime$beat)
head(altcrime)
plot(altcrime$crime, altcrime$beat)
plot(altcrime$crime, altcrime$neighborhood)
unique(altcrime$crime)
table
table(altcrime$crime)
plot(table(altcrime$crime))
#Load libraries,
require(ggplot2)
require(animation)
require(dplyr)
#Load data
edeaths <- read.csv('altcrime.csv')
#Load the map data, set color value to -1
s = map_data('world')
install.packages("maps")
#Load libraries,
require(ggplot2)
require(animation)
require(dplyr)
#Load data
edeaths <- read.csv('altcrime.csv')
#Load the map data, set color value to -1
s = map_data('world')
s
s$colour = -1
require(ggplot2)
require(animation)
require(dplyr)
install.packages("animation")
require(ggplot2)
require(animation)
require(dplyr)
#Load data
altcrime <- read.csv('altcrime.csv')
#Load the map data, set color value to -1
s = map_data('world')
s$colour = -1
head(altcrime)
#Load data
altcrime <- read.csv('altcrime.csv')
#Load the map data, set color value to -1
s = map_data('world')
s$colour = -1
#malaria estimated  deaths
#create a list of years for use in both plots
years <- altcrime$date
head(years)
saveGIF(
for (yr in years ){
print(yr)
altcrime<-read.csv("altcrimev1.csv")
head(altcrime)
plot(altcrime$year, altcrime$neighborhood)
altcrime<-read.csv("altcrimev1.csv")
head(altcrime)
plot(altcrime$year, altcrime$neighborhood)
altcrime<-read.csv("altcrimev1.csv")
head(altcrime)
plot(altcrime$year, altcrime$neighborhood)
altcrime<-read.csv("altcrimev1.csv")
altcrime<-read.csv("altcrimev1.csv")
head(altcrime)
plot(altcrime$year, altcrime$neighborhood)
unique(altcrime$year)
>>>>>>> c366098756d68c37760d2af766458b056d04171a
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
table(eddat$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NA'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#14,25
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:10)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 50)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))
table(eddat$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NA'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24, 14,25)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
table(dm$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NA'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
table(eddat$group)
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
head(eddat)
table(eddat$group)
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
table(eddat$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24, 14,25)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:12)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:12)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 50)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])$group)#,14,25
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:10)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 50)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
sapply(dm6, function(x) sum(!is.na(x))/length(x))
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])
dm$group <- as.factor(dm$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:10)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:10)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 50)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:12)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:12)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])$group)#
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:17)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:17)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:17)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:17)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:17)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:17)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:16)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])$group)#
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,11,15,18,21,29)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:17)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:17)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
# turn output on again
sink()
summary(Accuracies)
acclen = length(na.omit(Accuracies))
plot(density(Accuracies), main = paste('Average Neural Network Model accuracies:',acclen, 'runs') )
table(dm$group)
require(caret) #select tuning parameters
require(e1071) #SVM
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])$group)#,11,15,18,21,29
dm <- na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25)])
dm$group <- as.factor(dm$group)
dm <- cbind(dm["group"], scale(dm[,c(2:12)]) )
# time how long it takes
start_time <- Sys.time()
Accuracies <- c(0.00)
# suppress console output
sink('sinktest.txt')
for (i in seq(100))
{
# sample
inTrain <- createDataPartition(dm$group, p = .80, list = FALSE)
dm.navg <- avNNet(dm[c(2:12)], dm$group, subset = inTrain, size = 2, rang = 0.5, decay = 5e-4, maxit = 200, repeats = 20)
Accuracies[i] <- confusionMatrix(dm$group[-inTrain], predict(dm.navg, dm[-inTrain,], type = "class"))$overall["Accuracy"]
}
eddat <- read.csv('EdgarMDPraveen1.csv', as.is = T);
attach(eddat)
#six group
dm6 <- eddat[which(group == 'AfA' | group == 'AsA'| group == 'EA'| group == 'NAm'| group == 'HW'),];
colSums(is.na(eddat))
table(na.omit(dm6[,c(9,12,13,16,17,19,20,22,23,24,14,25,15,18,21,29)])$group)#,11,15,18,21,29
>>>>>>> b94920f3bcc5e5b3d822fa04d8a719d5008d5651
